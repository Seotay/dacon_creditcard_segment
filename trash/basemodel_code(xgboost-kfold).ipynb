{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import gc\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from utils.reduce_memory import trainform_columns_type\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.utils import resample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단일 값 가지는 컬럼 & NULL 값 가지는 컬럼 모두 제거."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train dataset(drop colums) (2400000, 743)\n",
      "shape of test dataset(drop colums) (600000, 742)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_parquet('./data/train/train_filtered.parquet')\n",
    "test_df = pd.read_parquet('./data/test/test_filtered.parquet')\n",
    "\n",
    "# reduce memory by chaing data types of columns\n",
    "train_df = trainform_columns_type(train_df)\n",
    "test_df = trainform_columns_type(test_df)\n",
    "\n",
    "print('shape of train dataset(drop colums)', train_df.shape)\n",
    "print('shape of test dataset(drop colums)', test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>기준년월</th>\n",
       "      <th>ID</th>\n",
       "      <th>남녀구분코드</th>\n",
       "      <th>연령</th>\n",
       "      <th>Segment</th>\n",
       "      <th>회원여부_이용가능</th>\n",
       "      <th>회원여부_이용가능_CA</th>\n",
       "      <th>회원여부_이용가능_카드론</th>\n",
       "      <th>소지여부_신용</th>\n",
       "      <th>소지카드수_유효_신용</th>\n",
       "      <th>...</th>\n",
       "      <th>변동률_RV일시불평잔</th>\n",
       "      <th>변동률_할부평잔</th>\n",
       "      <th>변동률_CA평잔</th>\n",
       "      <th>변동률_RVCA평잔</th>\n",
       "      <th>변동률_카드론평잔</th>\n",
       "      <th>변동률_잔액_B1M</th>\n",
       "      <th>변동률_잔액_일시불_B1M</th>\n",
       "      <th>변동률_잔액_CA_B1M</th>\n",
       "      <th>혜택수혜율_R3M</th>\n",
       "      <th>혜택수혜율_B0M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201807</td>\n",
       "      <td>TRAIN_000000</td>\n",
       "      <td>2</td>\n",
       "      <td>40대</td>\n",
       "      <td>D</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>1.042805</td>\n",
       "      <td>0.999700</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.261886</td>\n",
       "      <td>0.270752</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.044401</td>\n",
       "      <td>1.280542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201807</td>\n",
       "      <td>TRAIN_000001</td>\n",
       "      <td>1</td>\n",
       "      <td>30대</td>\n",
       "      <td>E</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.092698</td>\n",
       "      <td>0.905663</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>-0.563388</td>\n",
       "      <td>-0.670348</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201807</td>\n",
       "      <td>TRAIN_000002</td>\n",
       "      <td>1</td>\n",
       "      <td>30대</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.006124</td>\n",
       "      <td>1.993590</td>\n",
       "      <td>0.852567</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>-0.046516</td>\n",
       "      <td>0.058114</td>\n",
       "      <td>-0.014191</td>\n",
       "      <td>0.524159</td>\n",
       "      <td>1.208420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>201807</td>\n",
       "      <td>TRAIN_000003</td>\n",
       "      <td>2</td>\n",
       "      <td>40대</td>\n",
       "      <td>D</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>1.050646</td>\n",
       "      <td>0.999877</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.023821</td>\n",
       "      <td>0.258943</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.880925</td>\n",
       "      <td>1.657124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201807</td>\n",
       "      <td>TRAIN_000004</td>\n",
       "      <td>2</td>\n",
       "      <td>40대</td>\n",
       "      <td>E</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2399995</th>\n",
       "      <td>201812</td>\n",
       "      <td>TRAIN_399995</td>\n",
       "      <td>2</td>\n",
       "      <td>70대이상</td>\n",
       "      <td>E</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2399996</th>\n",
       "      <td>201812</td>\n",
       "      <td>TRAIN_399996</td>\n",
       "      <td>2</td>\n",
       "      <td>50대</td>\n",
       "      <td>D</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.921733</td>\n",
       "      <td>-0.203251</td>\n",
       "      <td>-0.159143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.377071</td>\n",
       "      <td>2.533815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2399997</th>\n",
       "      <td>201812</td>\n",
       "      <td>TRAIN_399997</td>\n",
       "      <td>1</td>\n",
       "      <td>30대</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.345027</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.027319</td>\n",
       "      <td>0.126581</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2399998</th>\n",
       "      <td>201812</td>\n",
       "      <td>TRAIN_399998</td>\n",
       "      <td>1</td>\n",
       "      <td>40대</td>\n",
       "      <td>E</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2399999</th>\n",
       "      <td>201812</td>\n",
       "      <td>TRAIN_399999</td>\n",
       "      <td>2</td>\n",
       "      <td>40대</td>\n",
       "      <td>E</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.593160</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>-0.039845</td>\n",
       "      <td>-0.002659</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2400000 rows × 743 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           기준년월            ID  남녀구분코드     연령 Segment  회원여부_이용가능  회원여부_이용가능_CA  \\\n",
       "0        201807  TRAIN_000000       2    40대       D          1             1   \n",
       "1        201807  TRAIN_000001       1    30대       E          1             1   \n",
       "2        201807  TRAIN_000002       1    30대       C          1             1   \n",
       "3        201807  TRAIN_000003       2    40대       D          1             1   \n",
       "4        201807  TRAIN_000004       2    40대       E          1             1   \n",
       "...         ...           ...     ...    ...     ...        ...           ...   \n",
       "2399995  201812  TRAIN_399995       2  70대이상       E          1             1   \n",
       "2399996  201812  TRAIN_399996       2    50대       D          1             1   \n",
       "2399997  201812  TRAIN_399997       1    30대       C          1             1   \n",
       "2399998  201812  TRAIN_399998       1    40대       E          1             1   \n",
       "2399999  201812  TRAIN_399999       2    40대       E          1             1   \n",
       "\n",
       "         회원여부_이용가능_카드론  소지여부_신용  소지카드수_유효_신용  ...  변동률_RV일시불평잔  변동률_할부평잔  \\\n",
       "0                    0        1            1  ...     0.999998  1.042805   \n",
       "1                    1        1            1  ...     1.092698  0.905663   \n",
       "2                    0        1            1  ...     1.006124  1.993590   \n",
       "3                    0        1            2  ...     0.999998  1.050646   \n",
       "4                    1        1            1  ...     0.999998  0.999998   \n",
       "...                ...      ...          ...  ...          ...       ...   \n",
       "2399995              1        1            1  ...     0.999998  0.999998   \n",
       "2399996              1        1            1  ...     0.999998  0.999998   \n",
       "2399997              0        1            1  ...     0.999998  0.345027   \n",
       "2399998              1        1            1  ...     0.999998  0.999998   \n",
       "2399999              0        1            1  ...     0.999998  0.593160   \n",
       "\n",
       "         변동률_CA평잔  변동률_RVCA평잔  변동률_카드론평잔  변동률_잔액_B1M  변동률_잔액_일시불_B1M  \\\n",
       "0        0.999700    0.999998   0.999998    0.261886        0.270752   \n",
       "1        0.999998    0.999998   0.999998   -0.563388       -0.670348   \n",
       "2        0.852567    0.999998   0.999998   -0.046516        0.058114   \n",
       "3        0.999877    0.999998   0.999998    0.023821        0.258943   \n",
       "4        0.999998    0.999998   0.999998    0.000000        0.000000   \n",
       "...           ...         ...        ...         ...             ...   \n",
       "2399995  0.999998    0.999998   0.999998    0.000000        0.000000   \n",
       "2399996  0.999998    0.999998   0.921733   -0.203251       -0.159143   \n",
       "2399997  0.999998    0.999998   0.999998    0.027319        0.126581   \n",
       "2399998  0.999998    0.999998   0.999998    0.000000        0.000000   \n",
       "2399999  0.999998    0.999998   0.999998   -0.039845       -0.002659   \n",
       "\n",
       "         변동률_잔액_CA_B1M  혜택수혜율_R3M  혜택수혜율_B0M  \n",
       "0             0.000000   1.044401   1.280542  \n",
       "1             0.000000   0.000000   0.000000  \n",
       "2            -0.014191   0.524159   1.208420  \n",
       "3             0.000000   0.880925   1.657124  \n",
       "4             0.000000        NaN        NaN  \n",
       "...                ...        ...        ...  \n",
       "2399995       0.000000        NaN        NaN  \n",
       "2399996       0.000000   1.377071   2.533815  \n",
       "2399997       0.000000   0.000000   0.000000  \n",
       "2399998       0.000000        NaN        NaN  \n",
       "2399999       0.000000   0.000000   0.000000  \n",
       "\n",
       "[2400000 rows x 743 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train dataset(drop colums to have null values) (2400000, 712)\n",
      "shape of test dataset(drop colums to have null values) (600000, 711)\n"
     ]
    }
   ],
   "source": [
    "# remove columns to have null values\n",
    "null_cols = train_df.columns[train_df.isnull().any()]\n",
    "train_df = train_df.drop(columns=null_cols)          \n",
    "test_df = test_df.drop(columns=null_cols, errors='ignore')\n",
    "print('shape of train dataset(drop colums to have null values)', train_df.shape)\n",
    "print('shape of test dataset(drop colums to have null values)', test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "기준년월                int32\n",
       "ID                 object\n",
       "남녀구분코드              int32\n",
       "연령                 object\n",
       "Segment            object\n",
       "                   ...   \n",
       "변동률_RVCA평잔        float32\n",
       "변동률_카드론평잔         float32\n",
       "변동률_잔액_B1M        float32\n",
       "변동률_잔액_일시불_B1M    float32\n",
       "변동률_잔액_CA_B1M     float32\n",
       "Length: 712, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split X, y in train datasets 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X: ID, Segement 제거\n",
    "# y: Segment 추출\n",
    "feature_cols = [col for col in train_df.columns if col not in [\"ID\", \"Segment\"]]\n",
    "\n",
    "X = train_df[feature_cols].copy() \n",
    "y = train_df[\"Segment\"].copy() \n",
    "\n",
    "# Target Label Encoding\n",
    "le_target = LabelEncoder()\n",
    "y_encoded = le_target.fit_transform(y)\n",
    "\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "X_test = test_df.copy()\n",
    "\n",
    "encoders = {}  # 각 컬럼별 encoder 저장\n",
    "\n",
    "for col in categorical_features:\n",
    "    le_train = LabelEncoder()\n",
    "    X[col] = le_train.fit_transform(X[col])\n",
    "    encoders[col] = le_train\n",
    "    unseen_labels_val = set(X_test[col]) - set(le_train.classes_)\n",
    "    if unseen_labels_val:\n",
    "        le_train.classes_ = np.append(le_train.classes_, list(unseen_labels_val))\n",
    "    X_test[col] = le_train.transform(X_test[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classes = np.unique(y_encoded)\n",
    "weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_encoded)\n",
    "class_weight_dict = dict(zip(classes, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 493.82716049382714,\n",
       " 1: 3333.3333333333335,\n",
       " 2: 3.7620503174229953,\n",
       " 3: 1.3744051402752246,\n",
       " 4: 0.2497330977517778}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weight_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.37440514, 3.76205032, 3.76205032, ..., 3.76205032, 3.76205032,\n",
       "       3.76205032])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_weights = np.array([class_weight_dict[y] for y in y_encoded])\n",
    "sample_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Fold 1-th XGBoost model training...\n",
      "Fold 1 Accuracy: 0.9080\n",
      "Fold 1 Recall: 0.8265\n",
      "Fold 1 Precision: 0.8378\n",
      "Fold 1 F1-score: 0.8194\n",
      "Fold 1 Classification Report\n",
      ":              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.92      0.83       201\n",
      "           1       0.95      0.91      0.93        45\n",
      "           2       0.68      0.81      0.74     25532\n",
      "           3       0.87      0.50      0.64     69969\n",
      "           4       0.93      0.99      0.96    384253\n",
      "\n",
      "    accuracy                           0.91    480000\n",
      "   macro avg       0.84      0.83      0.82    480000\n",
      "weighted avg       0.91      0.91      0.90    480000\n",
      "\n",
      "Fold 1 Confusion Matrix:\n",
      "[[   185      0     16      0      0]\n",
      " [     0     41      4      0      0]\n",
      " [    45      2  20658   2408   2419]\n",
      " [     9      0   8273  35234  26453]\n",
      " [     4      0   1646   2875 379728]]\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Fold 2-th XGBoost model training...\n",
      "Fold 2 Accuracy: 0.9080\n",
      "Fold 2 Recall: 0.8364\n",
      "Fold 2 Precision: 0.8164\n",
      "Fold 2 F1-score: 0.8102\n",
      "Fold 2 Classification Report\n",
      ":              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.93      0.77       169\n",
      "           1       0.95      0.95      0.95        20\n",
      "           2       0.68      0.81      0.74     25641\n",
      "           3       0.87      0.50      0.64     70156\n",
      "           4       0.93      0.99      0.96    384014\n",
      "\n",
      "    accuracy                           0.91    480000\n",
      "   macro avg       0.82      0.84      0.81    480000\n",
      "weighted avg       0.91      0.91      0.90    480000\n",
      "\n",
      "Fold 2 Confusion Matrix:\n",
      "[[   157      0     11      0      1]\n",
      " [     0     19      1      0      0]\n",
      " [    65      0  20827   2380   2369]\n",
      " [    11      0   8191  35233  26721]\n",
      " [     8      1   1608   2803 379594]]\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Fold 3-th XGBoost model training...\n",
      "Fold 3 Accuracy: 0.9088\n",
      "Fold 3 Recall: 0.8284\n",
      "Fold 3 Precision: 0.8172\n",
      "Fold 3 F1-score: 0.8084\n",
      "Fold 3 Classification Report\n",
      ":              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.93      0.80       179\n",
      "           1       0.91      0.91      0.91        32\n",
      "           2       0.68      0.81      0.74     25375\n",
      "           3       0.87      0.50      0.64     69752\n",
      "           4       0.93      0.99      0.96    384662\n",
      "\n",
      "    accuracy                           0.91    480000\n",
      "   macro avg       0.82      0.83      0.81    480000\n",
      "weighted avg       0.91      0.91      0.90    480000\n",
      "\n",
      "Fold 3 Confusion Matrix:\n",
      "[[   167      0     12      0      0]\n",
      " [     0     29      3      0      0]\n",
      " [    58      3  20540   2419   2355]\n",
      " [    11      0   8086  35199  26456]\n",
      " [     3      0   1530   2822 380307]]\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Fold 4-th XGBoost model training...\n",
      "Fold 4 Accuracy: 0.9074\n",
      "Fold 4 Recall: 0.8284\n",
      "Fold 4 Precision: 0.8441\n",
      "Fold 4 F1-score: 0.8231\n",
      "Fold 4 Classification Report\n",
      ":              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.91      0.82       208\n",
      "           1       1.00      0.94      0.97        16\n",
      "           2       0.67      0.81      0.74     25327\n",
      "           3       0.87      0.50      0.63     69890\n",
      "           4       0.93      0.99      0.96    384559\n",
      "\n",
      "    accuracy                           0.91    480000\n",
      "   macro avg       0.84      0.83      0.82    480000\n",
      "weighted avg       0.91      0.91      0.90    480000\n",
      "\n",
      "Fold 4 Confusion Matrix:\n",
      "[[   189      0     19      0      0]\n",
      " [     0     15      1      0      0]\n",
      " [    49      0  20493   2395   2390]\n",
      " [    10      0   8264  34832  26784]\n",
      " [     4      0   1646   2888 380021]]\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Fold 5-th XGBoost model training...\n",
      "Fold 5 Accuracy: 0.9091\n",
      "Fold 5 Recall: 0.7825\n",
      "Fold 5 Precision: 0.8279\n",
      "Fold 5 F1-score: 0.7867\n",
      "Fold 5 Classification Report\n",
      ":              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.93      0.80       215\n",
      "           1       0.95      0.68      0.79        31\n",
      "           2       0.68      0.81      0.74     25715\n",
      "           3       0.87      0.51      0.64     69475\n",
      "           4       0.93      0.99      0.96    384564\n",
      "\n",
      "    accuracy                           0.91    480000\n",
      "   macro avg       0.83      0.78      0.79    480000\n",
      "weighted avg       0.91      0.91      0.90    480000\n",
      "\n",
      "Fold 5 Confusion Matrix:\n",
      "[[   200      0     15      0      0]\n",
      " [     1     21      9      0      0]\n",
      " [    64      0  20787   2455   2409]\n",
      " [    15      0   8073  35296  26091]\n",
      " [     4      1   1601   2879 380079]]\n",
      "----------------------------------------\n",
      "K-Fold mean Accuracy: 0.9083\n",
      "K-Fold mean Recall: 0.8204\n",
      "K-Fold mean Precision: 0.8287\n",
      "K-Fold mean F1-score: 0.8095\n"
     ]
    }
   ],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "models = [] \n",
    "accuracy_scores = []\n",
    "recall_scores = []\n",
    "precision_scores = []\n",
    "fi_scores = []\n",
    "classification_reports = []\n",
    "\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y_encoded[train_idx], y_encoded[val_idx]\n",
    "    sample_weights_fold = sample_weights[train_idx]\n",
    "    print('-'*40)\n",
    "    print(f'Fold {fold + 1}-th XGBoost model training...')\n",
    "    \n",
    "    # XGBoost\n",
    "    model = xgb.XGBClassifier(\n",
    "        tree_method='gpu_hist',  # GPU mode\n",
    "        gpu_id=0,\n",
    "        random_state=42,\n",
    "        sample_weight=sample_weights_fold,\n",
    "        use_label_encoder=False)\n",
    "    \n",
    "    # training and validation mornitoring\n",
    "    model.fit(X_train, y_train, \n",
    "              sample_weight=sample_weights[train_idx]\n",
    "              )\n",
    "    models.append(model)\n",
    "\n",
    "    y_val_pred = model.predict(X_val)\n",
    "    accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    recall = recall_score(y_val, y_val_pred, average='macro')\n",
    "    precision = precision_score(y_val, y_val_pred, average='macro')\n",
    "    f1 = f1_score(y_val, y_val_pred, average='macro')\n",
    "    report = classification_report(y_val, y_val_pred)\n",
    "    conf_matrix = confusion_matrix(y_val, y_val_pred)\n",
    "    print(f\"Fold {fold + 1} Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Fold {fold + 1} Recall: {recall:.4f}\")\n",
    "    print(f\"Fold {fold + 1} Precision: {precision:.4f}\")\n",
    "    print(f\"Fold {fold + 1} F1-score: {f1:.4f}\")\n",
    "    print(f\"Fold {fold + 1} Classification Report\\n:{report}\")\n",
    "    print(f\"Fold {fold + 1} Confusion Matrix:\\n{conf_matrix}\")\n",
    "    print('-'*40)\n",
    "    \n",
    "    accuracy_scores.append(accuracy)\n",
    "    recall_scores.append(recall)\n",
    "    precision_scores.append(precision)\n",
    "    fi_scores.append(f1)\n",
    "    \n",
    "    \n",
    "print(f\"K-Fold mean Accuracy: {np.mean(accuracy_scores):.4f}\")\n",
    "print(f\"K-Fold mean Recall: {np.mean(recall_scores):.4f}\")\n",
    "print(f\"K-Fold mean Precision: {np.mean(precision_scores):.4f}\")\n",
    "print(f\"K-Fold mean F1-score: {np.mean(fi_scores):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['ID'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mX_test\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mID\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\taehyeok\\lib\\site-packages\\pandas\\core\\frame.py:5258\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   5110\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[0;32m   5111\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   5112\u001b[0m     labels: IndexLabel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5119\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   5120\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5121\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   5122\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[0;32m   5123\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5256\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[0;32m   5257\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5258\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   5259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5260\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5264\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5265\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   5266\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\taehyeok\\lib\\site-packages\\pandas\\core\\generic.py:4549\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[1;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[0;32m   4547\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   4548\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 4549\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[0;32m   4552\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\taehyeok\\lib\\site-packages\\pandas\\core\\generic.py:4591\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[1;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[0;32m   4589\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[0;32m   4590\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 4591\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4592\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[0;32m   4594\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[0;32m   4595\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ADMIN\\anaconda3\\envs\\taehyeok\\lib\\site-packages\\pandas\\core\\indexes\\base.py:6699\u001b[0m, in \u001b[0;36mIndex.drop\u001b[1;34m(self, labels, errors)\u001b[0m\n\u001b[0;32m   6697\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m   6698\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 6699\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(labels[mask])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   6700\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[0;32m   6701\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[1;31mKeyError\u001b[0m: \"['ID'] not found in axis\""
     ]
    }
   ],
   "source": [
    "X_test.drop(columns=['ID'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Soft Voting Inference Done.\n"
     ]
    }
   ],
   "source": [
    "n_classes = models[0].n_classes_ \n",
    "test_probabilities = np.zeros((len(X_test), n_classes)) # (600000, 5)\n",
    "\n",
    "for model in models:\n",
    "    test_probabilities += model.predict_proba(X_test) # (600000, 5)\n",
    "\n",
    "test_probabilities /= len(models)\n",
    "test_predictions = np.argmax(test_probabilities, axis=1)\n",
    "\n",
    "print('Soft Voting Inference Done.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred_labels = le_target.inverse_transform(test_predictions)\n",
    "\n",
    "# row 단위 예측 결과를 test_data에 추가\n",
    "test_data = test_df.copy()  # 원본 유지\n",
    "test_data[\"pred_label\"] = y_test_pred_labels\n",
    "\n",
    "submission = test_data.groupby(\"ID\")[\"pred_label\"] \\\n",
    "    .agg(lambda x: x.value_counts().idxmax()) \\\n",
    "    .reset_index()\n",
    "\n",
    "submission.columns = [\"ID\", \"Segment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Segment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_00000</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_00001</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_00002</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_00003</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_00004</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99995</th>\n",
       "      <td>TEST_99995</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99996</th>\n",
       "      <td>TEST_99996</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99997</th>\n",
       "      <td>TEST_99997</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99998</th>\n",
       "      <td>TEST_99998</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99999</th>\n",
       "      <td>TEST_99999</td>\n",
       "      <td>E</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               ID Segment\n",
       "0      TEST_00000       E\n",
       "1      TEST_00001       E\n",
       "2      TEST_00002       E\n",
       "3      TEST_00003       E\n",
       "4      TEST_00004       E\n",
       "...           ...     ...\n",
       "99995  TEST_99995       E\n",
       "99996  TEST_99996       E\n",
       "99997  TEST_99997       E\n",
       "99998  TEST_99998       C\n",
       "99999  TEST_99999       E\n",
       "\n",
       "[100000 rows x 2 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('./results/xgboost_K5fold(class_weight02).csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taehyeok",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
