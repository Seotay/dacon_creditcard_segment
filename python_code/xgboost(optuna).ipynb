{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import gc\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from utils.reduce_memory import trainform_columns_type\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "warnings.filterwarnings('ignore')\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 단일 값 가지는 컬럼 & NULL 값 가지는 컬럼 모두 제거."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train dataset(drop colums) (2400000, 743)\n",
      "shape of test dataset(drop colums) (600000, 742)\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.read_parquet('./data/train/train_filtered.parquet')\n",
    "test_df = pd.read_parquet('./data/test/test_filtered.parquet')\n",
    "\n",
    "# reduce memory by chaing data types of columns\n",
    "train_df = trainform_columns_type(train_df)\n",
    "test_df = trainform_columns_type(test_df)\n",
    "\n",
    "print('shape of train dataset(drop colums)', train_df.shape)\n",
    "print('shape of test dataset(drop colums)', test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>기준년월</th>\n",
       "      <th>ID</th>\n",
       "      <th>남녀구분코드</th>\n",
       "      <th>연령</th>\n",
       "      <th>Segment</th>\n",
       "      <th>회원여부_이용가능</th>\n",
       "      <th>회원여부_이용가능_CA</th>\n",
       "      <th>회원여부_이용가능_카드론</th>\n",
       "      <th>소지여부_신용</th>\n",
       "      <th>소지카드수_유효_신용</th>\n",
       "      <th>...</th>\n",
       "      <th>변동률_RV일시불평잔</th>\n",
       "      <th>변동률_할부평잔</th>\n",
       "      <th>변동률_CA평잔</th>\n",
       "      <th>변동률_RVCA평잔</th>\n",
       "      <th>변동률_카드론평잔</th>\n",
       "      <th>변동률_잔액_B1M</th>\n",
       "      <th>변동률_잔액_일시불_B1M</th>\n",
       "      <th>변동률_잔액_CA_B1M</th>\n",
       "      <th>혜택수혜율_R3M</th>\n",
       "      <th>혜택수혜율_B0M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>201807</td>\n",
       "      <td>TRAIN_000000</td>\n",
       "      <td>2</td>\n",
       "      <td>40대</td>\n",
       "      <td>D</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>1.042805</td>\n",
       "      <td>0.999700</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.261886</td>\n",
       "      <td>0.270752</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.044401</td>\n",
       "      <td>1.280542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>201807</td>\n",
       "      <td>TRAIN_000001</td>\n",
       "      <td>1</td>\n",
       "      <td>30대</td>\n",
       "      <td>E</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.092698</td>\n",
       "      <td>0.905663</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>-0.563388</td>\n",
       "      <td>-0.670348</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>201807</td>\n",
       "      <td>TRAIN_000002</td>\n",
       "      <td>1</td>\n",
       "      <td>30대</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.006124</td>\n",
       "      <td>1.993590</td>\n",
       "      <td>0.852567</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>-0.046516</td>\n",
       "      <td>0.058114</td>\n",
       "      <td>-0.014191</td>\n",
       "      <td>0.524159</td>\n",
       "      <td>1.208420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>201807</td>\n",
       "      <td>TRAIN_000003</td>\n",
       "      <td>2</td>\n",
       "      <td>40대</td>\n",
       "      <td>D</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>1.050646</td>\n",
       "      <td>0.999877</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.023821</td>\n",
       "      <td>0.258943</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.880925</td>\n",
       "      <td>1.657124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>201807</td>\n",
       "      <td>TRAIN_000004</td>\n",
       "      <td>2</td>\n",
       "      <td>40대</td>\n",
       "      <td>E</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2399995</th>\n",
       "      <td>201812</td>\n",
       "      <td>TRAIN_399995</td>\n",
       "      <td>2</td>\n",
       "      <td>70대이상</td>\n",
       "      <td>E</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2399996</th>\n",
       "      <td>201812</td>\n",
       "      <td>TRAIN_399996</td>\n",
       "      <td>2</td>\n",
       "      <td>50대</td>\n",
       "      <td>D</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.921733</td>\n",
       "      <td>-0.203251</td>\n",
       "      <td>-0.159143</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.377071</td>\n",
       "      <td>2.533815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2399997</th>\n",
       "      <td>201812</td>\n",
       "      <td>TRAIN_399997</td>\n",
       "      <td>1</td>\n",
       "      <td>30대</td>\n",
       "      <td>C</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.345027</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.027319</td>\n",
       "      <td>0.126581</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2399998</th>\n",
       "      <td>201812</td>\n",
       "      <td>TRAIN_399998</td>\n",
       "      <td>1</td>\n",
       "      <td>40대</td>\n",
       "      <td>E</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2399999</th>\n",
       "      <td>201812</td>\n",
       "      <td>TRAIN_399999</td>\n",
       "      <td>2</td>\n",
       "      <td>40대</td>\n",
       "      <td>E</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.593160</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>-0.039845</td>\n",
       "      <td>-0.002659</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2400000 rows × 743 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           기준년월            ID  남녀구분코드     연령 Segment  회원여부_이용가능  회원여부_이용가능_CA  \\\n",
       "0        201807  TRAIN_000000       2    40대       D          1             1   \n",
       "1        201807  TRAIN_000001       1    30대       E          1             1   \n",
       "2        201807  TRAIN_000002       1    30대       C          1             1   \n",
       "3        201807  TRAIN_000003       2    40대       D          1             1   \n",
       "4        201807  TRAIN_000004       2    40대       E          1             1   \n",
       "...         ...           ...     ...    ...     ...        ...           ...   \n",
       "2399995  201812  TRAIN_399995       2  70대이상       E          1             1   \n",
       "2399996  201812  TRAIN_399996       2    50대       D          1             1   \n",
       "2399997  201812  TRAIN_399997       1    30대       C          1             1   \n",
       "2399998  201812  TRAIN_399998       1    40대       E          1             1   \n",
       "2399999  201812  TRAIN_399999       2    40대       E          1             1   \n",
       "\n",
       "         회원여부_이용가능_카드론  소지여부_신용  소지카드수_유효_신용  ...  변동률_RV일시불평잔  변동률_할부평잔  \\\n",
       "0                    0        1            1  ...     0.999998  1.042805   \n",
       "1                    1        1            1  ...     1.092698  0.905663   \n",
       "2                    0        1            1  ...     1.006124  1.993590   \n",
       "3                    0        1            2  ...     0.999998  1.050646   \n",
       "4                    1        1            1  ...     0.999998  0.999998   \n",
       "...                ...      ...          ...  ...          ...       ...   \n",
       "2399995              1        1            1  ...     0.999998  0.999998   \n",
       "2399996              1        1            1  ...     0.999998  0.999998   \n",
       "2399997              0        1            1  ...     0.999998  0.345027   \n",
       "2399998              1        1            1  ...     0.999998  0.999998   \n",
       "2399999              0        1            1  ...     0.999998  0.593160   \n",
       "\n",
       "         변동률_CA평잔  변동률_RVCA평잔  변동률_카드론평잔  변동률_잔액_B1M  변동률_잔액_일시불_B1M  \\\n",
       "0        0.999700    0.999998   0.999998    0.261886        0.270752   \n",
       "1        0.999998    0.999998   0.999998   -0.563388       -0.670348   \n",
       "2        0.852567    0.999998   0.999998   -0.046516        0.058114   \n",
       "3        0.999877    0.999998   0.999998    0.023821        0.258943   \n",
       "4        0.999998    0.999998   0.999998    0.000000        0.000000   \n",
       "...           ...         ...        ...         ...             ...   \n",
       "2399995  0.999998    0.999998   0.999998    0.000000        0.000000   \n",
       "2399996  0.999998    0.999998   0.921733   -0.203251       -0.159143   \n",
       "2399997  0.999998    0.999998   0.999998    0.027319        0.126581   \n",
       "2399998  0.999998    0.999998   0.999998    0.000000        0.000000   \n",
       "2399999  0.999998    0.999998   0.999998   -0.039845       -0.002659   \n",
       "\n",
       "         변동률_잔액_CA_B1M  혜택수혜율_R3M  혜택수혜율_B0M  \n",
       "0             0.000000   1.044401   1.280542  \n",
       "1             0.000000   0.000000   0.000000  \n",
       "2            -0.014191   0.524159   1.208420  \n",
       "3             0.000000   0.880925   1.657124  \n",
       "4             0.000000        NaN        NaN  \n",
       "...                ...        ...        ...  \n",
       "2399995       0.000000        NaN        NaN  \n",
       "2399996       0.000000   1.377071   2.533815  \n",
       "2399997       0.000000   0.000000   0.000000  \n",
       "2399998       0.000000        NaN        NaN  \n",
       "2399999       0.000000   0.000000   0.000000  \n",
       "\n",
       "[2400000 rows x 743 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of train dataset(drop colums to have null values) (2400000, 712)\n",
      "shape of test dataset(drop colums to have null values) (600000, 711)\n"
     ]
    }
   ],
   "source": [
    "# remove columns to have null values\n",
    "null_cols = train_df.columns[train_df.isnull().any()]\n",
    "train_df = train_df.drop(columns=null_cols)          \n",
    "test_df = test_df.drop(columns=null_cols, errors='ignore')\n",
    "print('shape of train dataset(drop colums to have null values)', train_df.shape)\n",
    "print('shape of test dataset(drop colums to have null values)', test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "기준년월                int32\n",
       "ID                 object\n",
       "남녀구분코드              int32\n",
       "연령                 object\n",
       "Segment            object\n",
       "                   ...   \n",
       "변동률_RVCA평잔        float32\n",
       "변동률_카드론평잔         float32\n",
       "변동률_잔액_B1M        float32\n",
       "변동률_잔액_일시불_B1M    float32\n",
       "변동률_잔액_CA_B1M     float32\n",
       "Length: 712, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split X, y in train datasets 분리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X: ID, Segement 제거\n",
    "# y: Segment 추출\n",
    "feature_cols = [col for col in train_df.columns if col not in [\"ID\", \"Segment\"]]\n",
    "\n",
    "X = train_df[feature_cols].copy() \n",
    "y = train_df[\"Segment\"].copy() \n",
    "\n",
    "# Target Label Encoding\n",
    "le_target = LabelEncoder()\n",
    "y_encoded = le_target.fit_transform(y)\n",
    "\n",
    "categorical_features = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "X_test = test_df.copy()\n",
    "\n",
    "encoders = {}  # 각 컬럼별 encoder 저장\n",
    "\n",
    "for col in categorical_features:\n",
    "    le_train = LabelEncoder()\n",
    "    X[col] = le_train.fit_transform(X[col])\n",
    "    encoders[col] = le_train\n",
    "    unseen_labels_val = set(X_test[col]) - set(le_train.classes_)\n",
    "    if unseen_labels_val:\n",
    "        le_train.classes_ = np.append(le_train.classes_, list(unseen_labels_val))\n",
    "    X_test[col] = le_train.transform(X_test[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "classes = np.unique(y_encoded)\n",
    "weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_encoded)\n",
    "class_weight_dict = dict(zip(classes, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 493.82716049382714,\n",
       " 1: 3333.3333333333335,\n",
       " 2: 3.7620503174229953,\n",
       " 3: 1.3744051402752246,\n",
       " 4: 0.2497330977517778}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weight_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.37440514, 3.76205032, 3.76205032, ..., 3.76205032, 3.76205032,\n",
       "       3.76205032])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_weights = np.array([class_weight_dict[y] for y in y_encoded])\n",
    "sample_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Fold 1-th XGBoost model training...\n",
      "Fold 1 Accuracy: 0.9080\n",
      "Fold 1 Recall: 0.8265\n",
      "Fold 1 Precision: 0.8378\n",
      "Fold 1 F1-score: 0.8194\n",
      "Fold 1 Classification Report\n",
      ":              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.92      0.83       201\n",
      "           1       0.95      0.91      0.93        45\n",
      "           2       0.68      0.81      0.74     25532\n",
      "           3       0.87      0.50      0.64     69969\n",
      "           4       0.93      0.99      0.96    384253\n",
      "\n",
      "    accuracy                           0.91    480000\n",
      "   macro avg       0.84      0.83      0.82    480000\n",
      "weighted avg       0.91      0.91      0.90    480000\n",
      "\n",
      "Fold 1 Confusion Matrix:\n",
      "[[   185      0     16      0      0]\n",
      " [     0     41      4      0      0]\n",
      " [    45      2  20658   2408   2419]\n",
      " [     9      0   8273  35234  26453]\n",
      " [     4      0   1646   2875 379728]]\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Fold 2-th XGBoost model training...\n",
      "Fold 2 Accuracy: 0.9080\n",
      "Fold 2 Recall: 0.8364\n",
      "Fold 2 Precision: 0.8164\n",
      "Fold 2 F1-score: 0.8102\n",
      "Fold 2 Classification Report\n",
      ":              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.65      0.93      0.77       169\n",
      "           1       0.95      0.95      0.95        20\n",
      "           2       0.68      0.81      0.74     25641\n",
      "           3       0.87      0.50      0.64     70156\n",
      "           4       0.93      0.99      0.96    384014\n",
      "\n",
      "    accuracy                           0.91    480000\n",
      "   macro avg       0.82      0.84      0.81    480000\n",
      "weighted avg       0.91      0.91      0.90    480000\n",
      "\n",
      "Fold 2 Confusion Matrix:\n",
      "[[   157      0     11      0      1]\n",
      " [     0     19      1      0      0]\n",
      " [    65      0  20827   2380   2369]\n",
      " [    11      0   8191  35233  26721]\n",
      " [     8      1   1608   2803 379594]]\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Fold 3-th XGBoost model training...\n",
      "Fold 3 Accuracy: 0.9088\n",
      "Fold 3 Recall: 0.8284\n",
      "Fold 3 Precision: 0.8172\n",
      "Fold 3 F1-score: 0.8084\n",
      "Fold 3 Classification Report\n",
      ":              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.93      0.80       179\n",
      "           1       0.91      0.91      0.91        32\n",
      "           2       0.68      0.81      0.74     25375\n",
      "           3       0.87      0.50      0.64     69752\n",
      "           4       0.93      0.99      0.96    384662\n",
      "\n",
      "    accuracy                           0.91    480000\n",
      "   macro avg       0.82      0.83      0.81    480000\n",
      "weighted avg       0.91      0.91      0.90    480000\n",
      "\n",
      "Fold 3 Confusion Matrix:\n",
      "[[   167      0     12      0      0]\n",
      " [     0     29      3      0      0]\n",
      " [    58      3  20540   2419   2355]\n",
      " [    11      0   8086  35199  26456]\n",
      " [     3      0   1530   2822 380307]]\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Fold 4-th XGBoost model training...\n",
      "Fold 4 Accuracy: 0.9074\n",
      "Fold 4 Recall: 0.8284\n",
      "Fold 4 Precision: 0.8441\n",
      "Fold 4 F1-score: 0.8231\n",
      "Fold 4 Classification Report\n",
      ":              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      0.91      0.82       208\n",
      "           1       1.00      0.94      0.97        16\n",
      "           2       0.67      0.81      0.74     25327\n",
      "           3       0.87      0.50      0.63     69890\n",
      "           4       0.93      0.99      0.96    384559\n",
      "\n",
      "    accuracy                           0.91    480000\n",
      "   macro avg       0.84      0.83      0.82    480000\n",
      "weighted avg       0.91      0.91      0.90    480000\n",
      "\n",
      "Fold 4 Confusion Matrix:\n",
      "[[   189      0     19      0      0]\n",
      " [     0     15      1      0      0]\n",
      " [    49      0  20493   2395   2390]\n",
      " [    10      0   8264  34832  26784]\n",
      " [     4      0   1646   2888 380021]]\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Fold 5-th XGBoost model training...\n",
      "Fold 5 Accuracy: 0.9091\n",
      "Fold 5 Recall: 0.7825\n",
      "Fold 5 Precision: 0.8279\n",
      "Fold 5 F1-score: 0.7867\n",
      "Fold 5 Classification Report\n",
      ":              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.70      0.93      0.80       215\n",
      "           1       0.95      0.68      0.79        31\n",
      "           2       0.68      0.81      0.74     25715\n",
      "           3       0.87      0.51      0.64     69475\n",
      "           4       0.93      0.99      0.96    384564\n",
      "\n",
      "    accuracy                           0.91    480000\n",
      "   macro avg       0.83      0.78      0.79    480000\n",
      "weighted avg       0.91      0.91      0.90    480000\n",
      "\n",
      "Fold 5 Confusion Matrix:\n",
      "[[   200      0     15      0      0]\n",
      " [     1     21      9      0      0]\n",
      " [    64      0  20787   2455   2409]\n",
      " [    15      0   8073  35296  26091]\n",
      " [     4      1   1601   2879 380079]]\n",
      "----------------------------------------\n",
      "K-Fold mean Accuracy: 0.9083\n",
      "K-Fold mean Recall: 0.8204\n",
      "K-Fold mean Precision: 0.8287\n",
      "K-Fold mean F1-score: 0.8095\n"
     ]
    }
   ],
   "source": [
    "# Optuna Objective function\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "def xgboost_objective(trial, kfold=None):\n",
    "    params = {\n",
    "    \"n_estimators\" :  trial.suggest_int('n_estimators', 5000, 10000),\n",
    "    \"max_depth\" : trial.suggest_int('max_depth', 3, 30),\n",
    "    \"learning_rate\" : trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "    \"subsample\" : trial.suggest_float('subsample', 0.5, 1.0),\n",
    "    \"colsample_bytree\" : trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "    \"gamma\" : trial.suggest_float('gamma', 0, 5)\n",
    "    }\n",
    "\n",
    "    scores = []\n",
    "    \n",
    "    # K-Fold\n",
    "    if kfold != None:\n",
    "        for train_idx, valid_idx in kf.split(X):\n",
    "            X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "            y_train, y_valid = y[train_idx], y[valid_idx]\n",
    "            sample_weights_fold = sample_weights[train_idx]\n",
    "            \n",
    "            model = xgb.xgb.XGBClassifier(**params,\n",
    "                                        tree_method='gpu_hist',  # GPU mode\n",
    "                                        gpu_id=0,\n",
    "                                        random_state=42,\n",
    "                                        sample_weight=sample_weights_fold,\n",
    "                                        use_label_encoder=False)\n",
    "            \n",
    "            model.fit(X_train, y_train, sample_weight=sample_weights[train_idx])\n",
    "\n",
    "            preds = model.predict(X_valid)\n",
    "            f1 = f1_score(X_valid, preds, average='macro')\n",
    "            scores.append(f1)\n",
    "            \n",
    "        return np.mean(scores)\n",
    "\n",
    "# optuna    \n",
    "xgb_study = optuna.create_study(direction='maximize', sampler=TPESampler(seed=42))\n",
    "xgb_study.optimize(lambda trial: xgboost_objective(trial, kf), n_trials=30)\n",
    "\n",
    "# best hyperparameter\n",
    "best_xgb_params = xgb_study.best_params\n",
    "print(f\"Best XGBoost Parameters: {best_xgb_params}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "taehyeok",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
